#!/usr/bin/env python
"""
VPS æµ‹è¯„æ–‡ç« çˆ¬è™« - CLI å…¥å£
æ”¯æŒå¤šç«™ç‚¹çˆ¬å–ï¼Œè¾“å‡º JSON æˆ– Markdown æ ¼å¼
"""
import argparse
import sys
import os

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ Python è·¯å¾„ä¸­
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.scrapers import GWVPSScraper
from config import SCRAPE_CONFIG


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="VPS æµ‹è¯„æ–‡ç« çˆ¬è™« - çˆ¬å– VPS æµ‹è¯„ç½‘ç«™å¹¶æå–ç»“æ„åŒ–æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # çˆ¬å–é»˜è®¤ 1 é¡µï¼Œè¾“å‡º JSON
  python main.py
  
  # çˆ¬å– 3 é¡µ
  python main.py -p 3
  
  # çˆ¬å–å•ç¯‡æ–‡ç« 
  python main.py -u https://www.gwvpsceping.com/8785.html
  
  # è¾“å‡ºä¸º Markdown
  python main.py -f markdown
  
  # æŸ¥çœ‹æœ€è¿‘ 5 å¤©çš„æ–‡ç« 
  python main.py --recent 5
  
  # Pipeline: çˆ¬å–æœ€è¿‘æ–‡ç« å¹¶ç”¨ AI æ€»ç»“ä¸º JSON
  python main.py --pipeline 3
  
  # ä½¿ç”¨ uv è¿è¡Œ
  uv run python main.py -p 2
"""
    )
    
    parser.add_argument(
        "-p", "--pages",
        type=int,
        default=SCRAPE_CONFIG.get("default_pages", 1),
        help=f"çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤: {SCRAPE_CONFIG.get('default_pages', 1)}ï¼‰"
    )
    
    parser.add_argument(
        "-s", "--site",
        type=str,
        default="gwvps",
        choices=["gwvps"],
        help="ç›®æ ‡ç«™ç‚¹ï¼ˆé»˜è®¤: gwvpsï¼‰"
    )
    
    parser.add_argument(
        "-u", "--url",
        type=str,
        default=None,
        help="å•ç¯‡æ–‡ç«  URLï¼ˆæŒ‡å®šæ—¶å¿½ç•¥ --pagesï¼‰"
    )
    
    parser.add_argument(
        "-f", "--format",
        type=str,
        default="json",
        choices=["json", "markdown"],
        help="è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤: jsonï¼‰"
    )
    
    parser.add_argument(
        "-r", "--recent",
        type=int,
        default=None,
        metavar="DAYS",
        help="æŸ¥çœ‹æœ€è¿‘ N å¤©çš„æ–‡ç« åˆ—è¡¨ï¼ˆä½¿ç”¨å¤šçº¿ç¨‹ï¼‰"
    )
    
    parser.add_argument(
        "-t", "--threads",
        type=int,
        default=4,
        help="çˆ¬å–çº¿ç¨‹æ•°ï¼ˆç”¨äº --recent å’Œ --pipeline æ¨¡å¼ï¼Œé»˜è®¤: 4ï¼‰"
    )
    
    parser.add_argument(
        "--pipeline",
        type=int,
        default=None,
        metavar="DAYS",
        help="Pipeline æ¨¡å¼ï¼šçˆ¬å–æœ€è¿‘ N å¤©æ–‡ç« å¹¶ç”¨ AI æ€»ç»“ä¸º JSON"
    )
    
    parser.add_argument(
        "--ai-threads",
        type=int,
        default=2,
        help="AI å¤„ç†çº¿ç¨‹æ•°ï¼ˆä»…ç”¨äº --pipeline æ¨¡å¼ï¼Œé»˜è®¤: 2ï¼‰"
    )
    
    return parser


def get_scraper(site: str):
    """æ ¹æ®ç«™ç‚¹åç§°è·å–å¯¹åº”çš„çˆ¬è™«å®ä¾‹"""
    scrapers = {
        "gwvps": GWVPSScraper,
    }
    
    scraper_class = scrapers.get(site)
    if not scraper_class:
        print(f"âŒ ä¸æ”¯æŒçš„ç«™ç‚¹: {site}")
        print(f"   æ”¯æŒçš„ç«™ç‚¹: {', '.join(scrapers.keys())}")
        sys.exit(1)
    
    return scraper_class()


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()
    
    print("=" * 50)
    print("ğŸ•·ï¸  VPS æµ‹è¯„æ–‡ç« çˆ¬è™«")
    print("=" * 50)
    print(f"ç«™ç‚¹: {args.site}")
    
    # è·å–çˆ¬è™«å®ä¾‹
    scraper = get_scraper(args.site)
    
    # Pipeline æ¨¡å¼ï¼šçˆ¬å– + AI æ€»ç»“
    if args.pipeline is not None:
        print(f"æ¨¡å¼: Pipelineï¼ˆçˆ¬å– + AI æ€»ç»“ï¼‰")
        print(f"å¤©æ•°: {args.pipeline}")
        print(f"çˆ¬å–çº¿ç¨‹: {args.threads}")
        print(f"AI çº¿ç¨‹: {args.ai_threads}")
        print("=" * 50)
        print()
        
        results = scraper.pipeline_recent_to_json(
            days=args.pipeline,
            scrape_threads=args.threads,
            ai_threads=args.ai_threads
        )
        return
    
    # æœ€è¿‘æ–‡ç« æ¨¡å¼
    if args.recent is not None:
        print(f"æ¨¡å¼: æœ€è¿‘æ–‡ç« æŸ¥è¯¢")
        print(f"å¤©æ•°: {args.recent}")
        print(f"çº¿ç¨‹æ•°: {args.threads}")
        print("=" * 50)
        print()
        
        articles = scraper.print_recent_articles(
            days=args.recent,
            num_threads=args.threads
        )
        return
    
    # å¸¸è§„çˆ¬å–æ¨¡å¼
    print(f"è¾“å‡ºæ ¼å¼: {args.format}")
    
    if args.url:
        print(f"æ¨¡å¼: å•ç¯‡æ–‡ç« ")
        print(f"URL: {args.url}")
    else:
        print(f"æ¨¡å¼: åˆ—è¡¨çˆ¬å–")
        print(f"é¡µæ•°: {args.pages}")
    
    print("=" * 50)
    print()
    
    # è¿è¡Œçˆ¬è™«
    results = scraper.run(
        max_pages=args.pages,
        output_format=args.format,
        single_url=args.url
    )
    
    print()
    print("=" * 50)
    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {len(results)} ç¯‡æ–‡ç« ")
    print("=" * 50)


if __name__ == "__main__":
    main()
