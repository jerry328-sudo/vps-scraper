# é¡µé¢æå–æ–¹å¼ä½¿ç”¨æŒ‡å—

## æ¦‚è§ˆ

`GWVPSScraper` ç°åœ¨æ”¯æŒä¸¤ç§é¡µé¢å†…å®¹æå–æ–¹å¼ï¼š

1. **æ ‡å‡†çˆ¬è™«æ–¹å¼**ï¼ˆé»˜è®¤ï¼‰- ä½¿ç”¨ `requests` + `html_to_text`
2. **Tavily API æ–¹å¼** - ä½¿ç”¨ Tavily çš„æ™ºèƒ½æå– API

## é…ç½®

### ç¯å¢ƒå˜é‡

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ  Tavily API Keyï¼ˆå¯é€‰ï¼‰ï¼š

```env
# æ ‡å‡†é…ç½®
ZHIPU_API_KEY=your_zhipu_api_key

# å¦‚æœè¦ä½¿ç”¨ Tavily æå–æ–¹å¼ï¼Œæ·»åŠ æ­¤é…ç½®
TAVILY_API_KEY=your_tavily_api_key
```

## ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ 1: æ ‡å‡†çˆ¬è™«ï¼ˆé»˜è®¤ï¼‰

```python
from src.scrapers.gwvps_scraper import GWVPSScraper

# ä½¿ç”¨æ ‡å‡†çˆ¬è™«æ–¹å¼
scraper = GWVPSScraper(use_tavily=False)  # æˆ–ç›´æ¥ GWVPSScraper()

# çˆ¬å–å•ç¯‡æ–‡ç« 
result = scraper.scrape_with_ai("https://www.gwvpsceping.com/8810.html")
```

**ç‰¹ç‚¹ï¼š**
- âœ… ä¸éœ€è¦é¢å¤–çš„ API Key
- âœ… å®Œå…¨æ§åˆ¶çˆ¬å–è¿‡ç¨‹
- âœ… ä¿å­˜åŸå§‹ HTML æ–‡ä»¶
- âš ï¸ å¯èƒ½é‡åˆ°åçˆ¬è™«é™åˆ¶

### æ–¹å¼ 2: Tavily API æå–

```python
from src.scrapers.gwvps_scraper import GWVPSScraper

# ä½¿ç”¨ Tavily API æå–æ–¹å¼
scraper = GWVPSScraper(use_tavily=True)

# çˆ¬å–å•ç¯‡æ–‡ç« 
result = scraper.scrape_with_ai("https://www.gwvpsceping.com/8810.html")
```

**ç‰¹ç‚¹ï¼š**
- âœ… æ™ºèƒ½å†…å®¹æå–ï¼Œè´¨é‡æ›´é«˜
- âœ… æ›´å¥½çš„åçˆ¬è™«å¤„ç†
- âœ… è‡ªåŠ¨ fallback åˆ°æ ‡å‡†æ–¹å¼ï¼ˆå¦‚æœå¤±è´¥ï¼‰
- âš ï¸ éœ€è¦ Tavily API Keyï¼ˆä»˜è´¹æœåŠ¡ï¼‰

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: çˆ¬å–æœ€è¿‘æ–‡ç« ï¼ˆæ ‡å‡†æ–¹å¼ï¼‰

```python
from src.scrapers.gwvps_scraper import GWVPSScraper

# åˆå§‹åŒ–ï¼šæ ‡å‡†çˆ¬è™«
scraper = GWVPSScraper(use_tavily=False)

# çˆ¬å–æœ€è¿‘ 5 å¤©çš„æ–‡ç« å¹¶ç”¨ AI æå–
results = scraper.pipeline_recent_to_json(
    days=5,
    scrape_threads=4,  # çˆ¬å–æ–‡ç« åˆ—è¡¨ç”¨ 4 ä¸ªçº¿ç¨‹
    ai_threads=2,      # AI å¤„ç†ç”¨ 2 ä¸ªçº¿ç¨‹
    max_pages=50
)

print(f"æˆåŠŸæå– {len(results)} ç¯‡æ–‡ç« ")
```

### ç¤ºä¾‹ 2: çˆ¬å–æœ€è¿‘æ–‡ç« ï¼ˆTavily æ–¹å¼ï¼‰

```python
from src.scrapers.gwvps_scraper import GWVPSScraper

# åˆå§‹åŒ–ï¼šTavily API æå–
scraper = GWVPSScraper(use_tavily=True)

# çˆ¬å–æœ€è¿‘ 3 å¤©çš„æ–‡ç« 
results = scraper.pipeline_recent_to_json(
    days=3,
    scrape_threads=4,
    ai_threads=2,
    max_pages=30
)

print(f"æˆåŠŸæå– {len(results)} ç¯‡æ–‡ç« ")
```

### ç¤ºä¾‹ 3: å•ç¯‡æ–‡ç« å¯¹æ¯”æµ‹è¯•

```python
from src.scrapers.gwvps_scraper import GWVPSScraper

url = "https://www.gwvpsceping.com/8810.html"

# æ ‡å‡†æ–¹å¼
print("=" * 80)
print("ä½¿ç”¨æ ‡å‡†çˆ¬è™«æ–¹å¼")
print("=" * 80)
scraper_standard = GWVPSScraper(use_tavily=False)
result_standard = scraper_standard.scrape_with_ai(url)

# Tavily æ–¹å¼
print("\n" + "=" * 80)
print("ä½¿ç”¨ Tavily API æ–¹å¼")
print("=" * 80)
scraper_tavily = GWVPSScraper(use_tavily=True)
result_tavily = scraper_tavily.scrape_with_ai(url)

# å¯¹æ¯”ç»“æœ
print("\n" + "=" * 80)
print("ç»“æœå¯¹æ¯”")
print("=" * 80)
print(f"æ ‡å‡†æ–¹å¼æå–: {result_standard is not None}")
print(f"Tavily æ–¹å¼æå–: {result_tavily is not None}")
```

## è‡ªåŠ¨ Fallback æœºåˆ¶

å½“ä½¿ç”¨ `use_tavily=True` æ—¶ï¼Œå¦‚æœ Tavily API æå–å¤±è´¥ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ°æ ‡å‡†çˆ¬è™«æ–¹å¼ï¼š

```
ğŸ“¡ æ­£åœ¨çˆ¬å–: https://www.gwvpsceping.com/8810.html
ğŸ“¡ ä½¿ç”¨ Tavily API æå–é¡µé¢: https://...
âš ï¸  Tavily API è¿”å›ç»“æœä¸ºç©º
âš ï¸  Tavily æå–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ ‡å‡†çˆ¬è™«æ–¹å¼...
âœ… HTML è·å–æˆåŠŸï¼Œé•¿åº¦: 45678 å­—ç¬¦
ğŸ“ æ­£åœ¨å°† HTML è½¬æ¢ä¸ºçº¯æ–‡æœ¬...
   æå–æ–‡æœ¬é•¿åº¦: 12345 å­—ç¬¦
ğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹æå–ç»“æ„åŒ–æ•°æ®...
```

## æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | æ ‡å‡†çˆ¬è™« | Tavily API |
|------|---------|-----------|
| æå–è´¨é‡ | â­â­â­ | â­â­â­â­â­ |
| é€Ÿåº¦ | å¿« | ä¸­ç­‰ |
| æˆæœ¬ | å…è´¹ | ä»˜è´¹ |
| åçˆ¬è™«èƒ½åŠ› | ä½ | é«˜ |
| å¯é æ€§ | ä¾èµ–ç½‘ç«™ç»“æ„ | é«˜ |

## å»ºè®®

- **å¼€å‘æµ‹è¯•é˜¶æ®µ**ï¼šä½¿ç”¨æ ‡å‡†çˆ¬è™«ï¼ˆ`use_tavily=False`ï¼‰
- **ç”Ÿäº§ç¯å¢ƒ/å¤§è§„æ¨¡çˆ¬å–**ï¼šä½¿ç”¨ Tavily APIï¼ˆ`use_tavily=True`ï¼‰
- **ä¸ªäººé¡¹ç›®**ï¼šæ ‡å‡†çˆ¬è™«å³å¯æ»¡è¶³éœ€æ±‚
