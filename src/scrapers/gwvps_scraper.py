"""
ç‹—æ±ª VPS æµ‹è¯„ç½‘çˆ¬è™«
çˆ¬å– https://www.gwvpsceping.com/ çš„ VPS æµ‹è¯„æ–‡ç« 
"""
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.scrapers.base import BaseScraper
from src.ai_clients.zhipu_client import extract_vps_info
from src.utils import sanitize_filename, save_to_json, save_to_markdown, save_to_html
from config import TARGET_SITES, OUTPUT_CONFIG


class GWVPSScraper(BaseScraper):
    """
    ç‹—æ±ª VPS æµ‹è¯„ç½‘çˆ¬è™«å®ç°
    
    æ”¯æŒä¸¤ç§è¾“å‡ºæ ¼å¼ï¼š
    - JSON: ä½¿ç”¨ AI æå–ç»“æ„åŒ–æ•°æ®
    - Markdown: ä¿å­˜åŸå§‹æ–‡ç« å†…å®¹
    """
    
    def __init__(self):
        super().__init__(TARGET_SITES["gwvps"])
    
    def get_article_list(self, page: int = 1) -> List[Dict[str, str]]:
        """è·å–æŒ‡å®šé¡µçš„æ–‡ç« åˆ—è¡¨"""
        if page == 1:
            url = self.base_url
        else:
            url = f"{self.base_url}/page/{page}"
        
        print(f"ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page} é¡µæ–‡ç« åˆ—è¡¨: {url}")
        html = self._request(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, "html.parser")
        articles = []
        
        # ä½¿ç”¨é…ç½®çš„é€‰æ‹©å™¨æŸ¥æ‰¾æ–‡ç« é“¾æ¥
        selector = self.selectors.get("article_list", "h2 > a")
        for link in soup.select(selector):
            title = link.get_text(strip=True)
            href = link.get("href", "")
            if title and href:
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if not href.startswith("http"):
                    href = f"{self.base_url}/{href.lstrip('/')}"
                articles.append({"title": title, "link": href})
        
        print(f"   æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    def scrape_article(self, url: str) -> Optional[Dict[str, str]]:
        """çˆ¬å–å•ç¯‡æ–‡ç« å†…å®¹"""
        print(f"ğŸ“¡ æ­£åœ¨çˆ¬å–: {url}")
        html = self._request(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, "html.parser")
        
        # æå–æ ‡é¢˜
        title_selector = self.selectors.get("article_title", "h1")
        title_elem = soup.select_one(title_selector)
        title = title_elem.get_text(strip=True) if title_elem else "æ— æ ‡é¢˜"
        
        # æå–æ­£æ–‡
        content_selector = self.selectors.get("article_content", "article")
        content_elem = soup.select_one(content_selector)
        content = str(content_elem) if content_elem else ""
        
        return {
            "title": title,
            "content": content,
            "html": html,
            "url": url,
        }
    
    def scrape_with_ai(self, url: str) -> Optional[Dict]:
        """
        çˆ¬å–æ–‡ç« å¹¶ä½¿ç”¨ AI æå–ç»“æ„åŒ–æ•°æ®
        
        Args:
            url: æ–‡ç«  URL
            
        Returns:
            AI æå–çš„ç»“æ„åŒ–æ•°æ®å­—å…¸
        """
        print(f"ğŸ“¡ æ­£åœ¨çˆ¬å–: {url}")
        html = self._request(url)
        if not html:
            return None
        
        print(f"âœ… HTML è·å–æˆåŠŸï¼Œé•¿åº¦: {len(html)} å­—ç¬¦")
        
        # è°ƒç”¨ AI æå–ç»“æ„åŒ–æ•°æ®
        print("ğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹æå–ç»“æ„åŒ–æ•°æ®...")
        vps_info = extract_vps_info(html)
        
        if vps_info:
            vps_info["source_url"] = url
            return vps_info
        
        return None
    
    def run(
        self, 
        max_pages: int = 1, 
        output_format: str = "json",
        single_url: Optional[str] = None
    ) -> List[Dict]:
        """
        è¿è¡Œçˆ¬è™«
        
        Args:
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            output_format: è¾“å‡ºæ ¼å¼ï¼Œ"json" æˆ– "markdown"
            single_url: å•ç¯‡æ–‡ç«  URLï¼ˆæŒ‡å®šæ—¶å¿½ç•¥ max_pagesï¼‰
            
        Returns:
            çˆ¬å–ç»“æœåˆ—è¡¨
        """
        results = []
        
        # å•ç¯‡æ–‡ç« æ¨¡å¼
        if single_url:
            if output_format == "json":
                result = self.scrape_with_ai(single_url)
            else:
                result = self.scrape_article(single_url)
            
            if result:
                self._save_result(result, output_format)
                results.append(result)
            return results
        
        # åˆ—è¡¨çˆ¬å–æ¨¡å¼
        for page in range(1, max_pages + 1):
            articles = self.get_article_list(page)
            
            for article in articles:
                self._delay()
                
                if output_format == "json":
                    result = self.scrape_with_ai(article["link"])
                else:
                    result = self.scrape_article(article["link"])
                
                if result:
                    self._save_result(result, output_format)
                    results.append(result)
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼Œå…± {len(results)} ç¯‡æ–‡ç« ")
        return results
    
    def _save_result(self, result: Dict, output_format: str) -> None:
        """ä¿å­˜çˆ¬å–ç»“æœåˆ°æ–‡ä»¶"""
        if output_format == "json":
            # JSON æ ¼å¼ï¼šä» URL æå–æ–‡ä»¶å
            url = result.get("source_url", "")
            filename = url.split("/")[-1].replace(".html", "")
            filename = sanitize_filename(filename) or "article"
            output_dir = OUTPUT_CONFIG["raw_dir"]
            save_to_json(result, filename, output_dir)
            vendor = result.get("vendor", "æœªçŸ¥")
            product = result.get("product_name", "æœªçŸ¥")
            print(f"   å•†å®¶: {vendor} | äº§å“: {product}")
        else:
            # Markdown æ ¼å¼
            title = result.get("title", "æ— æ ‡é¢˜")
            filename = sanitize_filename(title)
            output_dir = OUTPUT_CONFIG["articles_dir"]
            save_to_markdown(
                title=title,
                content=result.get("content", ""),
                url=result.get("url", ""),
                filename=filename,
                output_dir=output_dir
            )

    def _get_articles_with_date_from_page(self, page: int) -> List[Dict[str, str]]:
        """
        è·å–æŒ‡å®šé¡µçš„æ–‡ç« åˆ—è¡¨ï¼ˆåŒ…å«æ—¥æœŸï¼‰
        
        Args:
            page: é¡µç 
            
        Returns:
            åŒ…å« title, link, date çš„æ–‡ç« åˆ—è¡¨
        """
        if page == 1:
            url = self.base_url
        else:
            url = f"{self.base_url}/page/{page}"
        
        html = self._request(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, "html.parser")
        articles = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« å…ƒç´ 
        for article_elem in soup.select("article"):
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            link_elem = article_elem.select_one("h2 > a")
            if not link_elem:
                continue
            
            title = link_elem.get_text(strip=True)
            href = link_elem.get("href", "")
            if not title or not href:
                continue
            
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if not href.startswith("http"):
                href = f"{self.base_url}/{href.lstrip('/')}"
            
            # æå–æ—¥æœŸ
            time_elem = article_elem.select_one("time")
            date_str = time_elem.get_text(strip=True) if time_elem else ""
            
            articles.append({
                "title": title,
                "link": href,
                "date": date_str
            })
        
        return articles

    def _fetch_page_worker(self, page: int, cutoff_date: datetime, results: List, lock: threading.Lock, stop_event: threading.Event) -> bool:
        """
        çº¿ç¨‹å·¥ä½œå‡½æ•°ï¼šè·å–å•é¡µæ–‡ç« å¹¶è¿‡æ»¤æ—¥æœŸ
        
        Args:
            page: é¡µç 
            cutoff_date: æˆªæ­¢æ—¥æœŸï¼ˆåªä¿ç•™æ­¤æ—¥æœŸä¹‹åçš„æ–‡ç« ï¼‰
            results: å…±äº«ç»“æœåˆ—è¡¨
            lock: çº¿ç¨‹é”
            stop_event: åœæ­¢äº‹ä»¶
            
        Returns:
            æ˜¯å¦åº”è¯¥ç»§ç»­æŠ“å–ï¼ˆTrue=ç»§ç»­ï¼ŒFalse=åœæ­¢ï¼‰
        """
        if stop_event.is_set():
            return False
        
        articles = self._get_articles_with_date_from_page(page)
        
        page_has_recent = False
        filtered_articles = []
        
        for article in articles:
            date_str = article.get("date", "")
            if date_str:
                try:
                    article_date = datetime.strptime(date_str, "%Y-%m-%d")
                    if article_date >= cutoff_date:
                        filtered_articles.append(article)
                        page_has_recent = True
                except ValueError:
                    # æ—¥æœŸæ ¼å¼è§£æå¤±è´¥ï¼Œè·³è¿‡
                    pass
        
        with lock:
            results.extend(filtered_articles)
        
        # å¦‚æœæ•´é¡µéƒ½æ²¡æœ‰ç¬¦åˆæ—¥æœŸæ¡ä»¶çš„æ–‡ç« ï¼Œè¯´æ˜åç»­é¡µä¹Ÿä¸ä¼šæœ‰äº†
        if not page_has_recent and articles:
            stop_event.set()
            return False
        
        return True

    def get_recent_articles(
        self, 
        days: int = 5, 
        max_pages: int = 50,
        num_threads: int = 4
    ) -> List[Dict[str, str]]:
        """
        å¤šçº¿ç¨‹è·å–æœ€è¿‘ N å¤©å†…å‘å¸ƒçš„æ–‡ç« 
        
        Args:
            days: æœ€è¿‘å¤©æ•°ï¼ˆé»˜è®¤ 5 å¤©ï¼‰
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤ 50 é¡µï¼Œé¿å…æ— é™çˆ¬å–ï¼‰
            num_threads: çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 4ï¼‰
            
        Returns:
            åŒ…å« title, link, date çš„æ–‡ç« åˆ—è¡¨ï¼ŒæŒ‰æ—¥æœŸé™åºæ’åˆ—
        """
        cutoff_date = datetime.now() - timedelta(days=days)
        cutoff_str = cutoff_date.strftime("%Y-%m-%d")
        
        print(f"ğŸ” å¼€å§‹è·å–æœ€è¿‘ {days} å¤©çš„æ–‡ç« ï¼ˆ{cutoff_str} ä¹‹åï¼‰")
        print(f"   ä½¿ç”¨ {num_threads} ä¸ªçº¿ç¨‹ï¼Œæœ€å¤§çˆ¬å– {max_pages} é¡µ")
        
        results: List[Dict] = []
        lock = threading.Lock()
        stop_event = threading.Event()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è·å–
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            # åˆ†æ‰¹æäº¤ä»»åŠ¡ï¼Œæ¯æ‰¹ num_threads ä¸ªé¡µé¢
            page = 1
            while page <= max_pages and not stop_event.is_set():
                # æäº¤ä¸€æ‰¹ä»»åŠ¡
                futures = {}
                for i in range(num_threads):
                    current_page = page + i
                    if current_page > max_pages:
                        break
                    future = executor.submit(
                        self._fetch_page_worker,
                        current_page,
                        cutoff_date,
                        results,
                        lock,
                        stop_event
                    )
                    futures[future] = current_page
                
                # ç­‰å¾…è¿™æ‰¹ä»»åŠ¡å®Œæˆ
                for future in as_completed(futures):
                    page_num = futures[future]
                    try:
                        future.result()
                        print(f"   âœ“ ç¬¬ {page_num} é¡µå®Œæˆï¼Œå½“å‰å…± {len(results)} ç¯‡æ–‡ç« ")
                    except Exception as e:
                        print(f"   âœ— ç¬¬ {page_num} é¡µå‡ºé”™: {e}")
                
                page += num_threads
                
                # å¦‚æœæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºå¾ªç¯
                if stop_event.is_set():
                    print("   ğŸ“Œ æ£€æµ‹åˆ°æ—§æ–‡ç« ï¼Œåœæ­¢ç»§ç»­çˆ¬å–")
                    break
        
        # æŒ‰æ—¥æœŸé™åºæ’åº
        results.sort(key=lambda x: x.get("date", ""), reverse=True)
        
        # å»é‡ï¼ˆåŸºäºé“¾æ¥ï¼‰
        seen_links = set()
        unique_results = []
        for article in results:
            if article["link"] not in seen_links:
                seen_links.add(article["link"])
                unique_results.append(article)
        
        print(f"\nâœ… å®Œæˆï¼æ‰¾åˆ° {len(unique_results)} ç¯‡æœ€è¿‘ {days} å¤©çš„æ–‡ç« \n")
        
        return unique_results

    def print_recent_articles(
        self,
        days: int = 5,
        max_pages: int = 50,
        num_threads: int = 4
    ) -> List[Dict[str, str]]:
        """
        è·å–å¹¶æ‰“å°æœ€è¿‘ N å¤©çš„æ–‡ç« åˆ—è¡¨
        
        Args:
            days: æœ€è¿‘å¤©æ•°ï¼ˆé»˜è®¤ 5 å¤©ï¼‰
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤ 50 é¡µï¼‰
            num_threads: çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 4ï¼‰
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        articles = self.get_recent_articles(days, max_pages, num_threads)
        
        if not articles:
            print("ğŸ˜• æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
            return []
        
        print("=" * 80)
        print(f"ğŸ“° æœ€è¿‘ {days} å¤©çš„æ–‡ç« åˆ—è¡¨ï¼ˆå…± {len(articles)} ç¯‡ï¼‰")
        print("=" * 80)
        
        for i, article in enumerate(articles, 1):
            print(f"\n{i}. [{article['date']}] {article['title']}")
            print(f"   ğŸ”— {article['link']}")
        
        print("\n" + "=" * 80)
        
        return articles

    def _ai_process_worker(
        self, 
        article: Dict[str, str], 
        results: List[Dict], 
        lock: threading.Lock,
        index: int,
        total: int
    ) -> Optional[Dict]:
        """
        AI å¤„ç†å·¥ä½œçº¿ç¨‹ï¼šçˆ¬å–å•ç¯‡æ–‡ç« å¹¶ç”¨ AI æå–ç»“æ„åŒ–æ•°æ®
        
        Args:
            article: æ–‡ç« ä¿¡æ¯ï¼ˆåŒ…å« title, link, dateï¼‰
            results: å…±äº«ç»“æœåˆ—è¡¨
            lock: çº¿ç¨‹é”
            index: å½“å‰ç´¢å¼•
            total: æ€»æ•°
        """
        url = article["link"]
        title = article["title"]
        
        print(f"   [{index}/{total}] ğŸ¤– æ­£åœ¨å¤„ç†: {title[:40]}...")
        
        try:
            html = self._request(url)
            if not html:
                print(f"   [{index}/{total}] âŒ è·å–å¤±è´¥: {title[:30]}...")
                return None
            
            # ä¿å­˜åŸå§‹ HTML é¡µé¢
            filename = url.split("/")[-1].replace(".html", "")
            filename = sanitize_filename(filename) or "article"
            save_to_html(html, filename, OUTPUT_CONFIG["html_dir"])
            
            # è°ƒç”¨ AI æå–ç»“æ„åŒ–æ•°æ®
            vps_info = extract_vps_info(html)
            
            if vps_info:
                vps_info["source_url"] = url
                vps_info["publish_date"] = article.get("date", "")
                
                # ä¿å­˜ JSON
                save_to_json(vps_info, filename, OUTPUT_CONFIG["raw_dir"])
                
                with lock:
                    results.append(vps_info)
                
                vendor = vps_info.get("vendor", "æœªçŸ¥")
                print(f"   [{index}/{total}] âœ… å®Œæˆ: {vendor} - {title[:30]}...")
                return vps_info
            else:
                print(f"   [{index}/{total}] âš ï¸ AI æå–å¤±è´¥: {title[:30]}...")
                return None
                
        except Exception as e:
            print(f"   [{index}/{total}] âŒ å‡ºé”™: {e}")
            return None

    def pipeline_recent_to_json(
        self,
        days: int = 5,
        scrape_threads: int = 4,
        ai_threads: int = 2,
        max_pages: int = 50
    ) -> List[Dict]:
        """
        Pipeline: çˆ¬å–æœ€è¿‘æ–‡ç« å¹¶ç”¨ AI æ€»ç»“ä¸º JSON
        
        æµç¨‹:
        1. å¤šçº¿ç¨‹çˆ¬å–æœ€è¿‘ N å¤©çš„æ–‡ç« åˆ—è¡¨ï¼ˆscrape_threads ä¸ªçº¿ç¨‹ï¼‰
        2. å¤šçº¿ç¨‹è°ƒç”¨ AI å¤„ç†æ¯ç¯‡æ–‡ç« ï¼ˆai_threads ä¸ªçº¿ç¨‹ï¼‰
        3. ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ° JSON æ–‡ä»¶
        
        Args:
            days: æœ€è¿‘å¤©æ•°ï¼ˆé»˜è®¤ 5 å¤©ï¼‰
            scrape_threads: çˆ¬å–çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 4ï¼‰
            ai_threads: AI å¤„ç†çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 2ï¼‰
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤ 50ï¼‰
            
        Returns:
            AI æå–çš„ç»“æ„åŒ–æ•°æ®åˆ—è¡¨
        """
        print("=" * 80)
        print("ğŸš€ Pipeline: çˆ¬å–æœ€è¿‘æ–‡ç«  â†’ AI æå– â†’ ä¿å­˜ JSON")
        print("=" * 80)
        print(f"   æ—¥æœŸèŒƒå›´: æœ€è¿‘ {days} å¤©")
        print(f"   çˆ¬å–çº¿ç¨‹: {scrape_threads}")
        print(f"   AI çº¿ç¨‹: {ai_threads}")
        print("=" * 80)
        print()
        
        # ç¬¬ä¸€æ­¥ï¼šå¤šçº¿ç¨‹è·å–æœ€è¿‘æ–‡ç« åˆ—è¡¨
        print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–æœ€è¿‘æ–‡ç« åˆ—è¡¨")
        print("-" * 40)
        articles = self.get_recent_articles(
            days=days, 
            max_pages=max_pages, 
            num_threads=scrape_threads
        )
        
        if not articles:
            print("ğŸ˜• æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
            return []
        
        print(f"\nğŸ“° å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« å¾…å¤„ç†")
        print()
        
        # ç¬¬äºŒæ­¥ï¼šå¤šçº¿ç¨‹ AI å¤„ç†
        print("ğŸ¤– ç¬¬äºŒæ­¥ï¼šAI æå–ç»“æ„åŒ–æ•°æ®")
        print("-" * 40)
        
        results: List[Dict] = []
        lock = threading.Lock()
        total = len(articles)
        
        with ThreadPoolExecutor(max_workers=ai_threads) as executor:
            futures = {}
            for i, article in enumerate(articles, 1):
                future = executor.submit(
                    self._ai_process_worker,
                    article,
                    results,
                    lock,
                    i,
                    total
                )
                futures[future] = article
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    article = futures[future]
                    print(f"   âŒ å¤„ç†å‡ºé”™ [{article['title'][:30]}]: {e}")
        
        # ç»Ÿè®¡ç»“æœ
        print()
        print("=" * 80)
        print(f"âœ… Pipeline å®Œæˆï¼")
        print(f"   æ–‡ç« æ€»æ•°: {total}")
        print(f"   æˆåŠŸå¤„ç†: {len(results)}")
        print(f"   å¤±è´¥æ•°é‡: {total - len(results)}")
        print(f"   è¾“å‡ºç›®å½•: {OUTPUT_CONFIG['raw_dir']}")
        print("=" * 80)
        
        return results

