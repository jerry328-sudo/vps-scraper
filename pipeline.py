#!/usr/bin/env python
"""
VPS æµ‹è¯„æ–‡ç« çˆ¬è™« - Pipeline è„šæœ¬
çˆ¬å–æœ€è¿‘æ–‡ç« å¹¶ç”¨ AI æ€»ç»“ä¸º JSON

ç›´æ¥è¿è¡Œ: python pipeline.py
"""
import sys
import os
import shutil
from datetime import datetime

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ Python è·¯å¾„ä¸­
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.scrapers import GWVPSScraper

# ============================================================
# å‚æ•°é…ç½®ï¼ˆåœ¨æ­¤å¤„ä¿®æ”¹ï¼‰
# ============================================================

# çˆ¬å–æœ€è¿‘ N å¤©çš„æ–‡ç« 
DAYS = 5

# çˆ¬å–æ–‡ç« åˆ—è¡¨çš„çº¿ç¨‹æ•°
SCRAPE_THREADS = 4

# AI å¤„ç†çš„çº¿ç¨‹æ•°
AI_THREADS = 5

# æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé˜²æ­¢æ— é™çˆ¬å–ï¼‰
MAX_PAGES = 50

# é¡µé¢æå–æ–¹å¼
# - False: æ ‡å‡†çˆ¬è™«ï¼ˆrequests + html_to_textï¼‰- å…è´¹ï¼Œå¯èƒ½é‡åˆ°åçˆ¬
# - True:  Tavily API æå– - éœ€è¦ API Keyï¼Œè´¨é‡æ›´é«˜ï¼Œæ›´ç¨³å®š
USE_TAVILY = True

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
OLD_DATA_DIR = os.path.join(DATA_DIR, "old")

# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================

def archive_old_data():
    """
    å°†æ—§æ•°æ®ç§»å…¥ data/old æ–‡ä»¶å¤¹
    æŒ‰æ—¶é—´æˆ³åˆ›å»ºå­ç›®å½•ï¼Œä¿ç•™å†å²æ•°æ®
    """
    # éœ€è¦å½’æ¡£çš„ç›®å½•
    dirs_to_archive = ["raw", "html", "articles"]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®éœ€è¦å½’æ¡£
    has_data = False
    for dir_name in dirs_to_archive:
        dir_path = os.path.join(DATA_DIR, dir_name)
        if os.path.exists(dir_path) and os.listdir(dir_path):
            has_data = True
            break
    
    if not has_data:
        print("ğŸ“‚ æ²¡æœ‰æ—§æ•°æ®éœ€è¦å½’æ¡£")
        return
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å½’æ¡£ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    archive_dir = os.path.join(OLD_DATA_DIR, timestamp)
    os.makedirs(archive_dir, exist_ok=True)
    
    print(f"ğŸ“¦ æ­£åœ¨å½’æ¡£æ—§æ•°æ®åˆ°: data/old/{timestamp}/")
    
    moved_count = 0
    for dir_name in dirs_to_archive:
        src_dir = os.path.join(DATA_DIR, dir_name)
        if not os.path.exists(src_dir):
            continue
        
        files = os.listdir(src_dir)
        if not files:
            continue
        
        # åˆ›å»ºç›®æ ‡ç›®å½•
        dest_dir = os.path.join(archive_dir, dir_name)
        os.makedirs(dest_dir, exist_ok=True)
        
        # ç§»åŠ¨æ–‡ä»¶
        for filename in files:
            src_file = os.path.join(src_dir, filename)
            dest_file = os.path.join(dest_dir, filename)
            if os.path.isfile(src_file):
                shutil.move(src_file, dest_file)
                moved_count += 1
    
    print(f"âœ… å·²å½’æ¡£ {moved_count} ä¸ªæ–‡ä»¶")
    print()

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    """è¿è¡Œ Pipeline"""
    print("=" * 60)
    print("ğŸš€ VPS æµ‹è¯„æ–‡ç« çˆ¬è™« - Pipeline")
    print("=" * 60)
    print(f"   æ—¥æœŸèŒƒå›´: æœ€è¿‘ {DAYS} å¤©")
    print(f"   çˆ¬å–çº¿ç¨‹: {SCRAPE_THREADS}")
    print(f"   AI çº¿ç¨‹:  {AI_THREADS}")
    print(f"   æœ€å¤§é¡µæ•°: {MAX_PAGES}")
    print(f"   æå–æ–¹å¼: {'Tavily API' if USE_TAVILY else 'æ ‡å‡†çˆ¬è™«'}")
    print("=" * 60)
    print()
    
    # æ£€æŸ¥å¿…éœ€çš„ API Key
    from config import API_KEYS
    
    zhipu_key = API_KEYS.get("zhipu", "")
    tavily_key = API_KEYS.get("tavily", "")
    
    if not zhipu_key:
        print("âŒ é”™è¯¯: æœªé…ç½® ZHIPU_API_KEY")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
        print("   è·å–åœ°å€: https://open.bigmodel.cn/")
        sys.exit(1)
    
    if USE_TAVILY and not tavily_key:
        print("âŒ é”™è¯¯: USE_TAVILY=True ä½†æœªé…ç½® TAVILY_API_KEY")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
        print("   è·å–åœ°å€: https://tavily.com/")
        sys.exit(1)
    
    # å½’æ¡£æ—§æ•°æ®
    archive_old_data()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = GWVPSScraper(use_tavily=USE_TAVILY)
    
    # è¿è¡Œ Pipeline
    results = scraper.pipeline_recent_to_json(
        days=DAYS,
        scrape_threads=SCRAPE_THREADS,
        ai_threads=AI_THREADS,
        max_pages=MAX_PAGES
    )
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    if results:
        print()
        print("ğŸ“Š å¤„ç†ç»“æœæ‘˜è¦:")
        print("-" * 40)
        for i, result in enumerate(results, 1):
            vendor = result.get("vendor", "æœªçŸ¥")
            product = result.get("product_name", "æœªçŸ¥")
            date = result.get("publish_date", "")
            print(f"   {i}. [{date}] {vendor} - {product}")
    
    return results


if __name__ == "__main__":
    main()
